{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"lstm_pve_model.ipynb","provenance":[],"authorship_tag":"ABX9TyNNBJg7sOeH3q2KowuZwKsf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"URm3A9uTZp6V"},"source":["#Import data with duplicates removed\n","#group exercises with the same input vector\n","#partition for validation\n","#save for use in other models for consistency and comparability\n","\n","import pickle\n","\n","with open('/filepath/lhs_dataset_old_assumptions_prepped_rmdup.txt', 'rb') as read:\n","  rm_dup = pickle.load(read)\n","\n","#Unzip data into X and Y\n","def unzip(dataset):\n","  dataset_X = []\n","  dataset_Y = []\n","  for exercise in dataset:\n","    temp_X, temp_Y = list(zip(*exercise))\n","    dataset_X.append(temp_X)\n","    dataset_Y.append(temp_Y)\n","  return dataset_X, dataset_Y\n","\n","lhs_dataset_X, lhs_dataset_Y = unzip(rm_dup)\n","\n","#Manual validation split - keep exercises with the same input X together\n","index_tracker = []\n","same_X = {}\n","\n","for i in range(len(lhs_dataset_X)):\n","  if i in index_tracker:\n","    continue\n","  else:\n","    index_tracker.append(i)\n","    same_X[i] = [i]\n","    for j in range(i+1,len(lhs_dataset_X)):\n","      if lhs_dataset_X[i] == lhs_dataset_X[j]:\n","        index_tracker.append(j)\n","        same_X[i].append(j)\n","\n","print(len(index_tracker))\n","print(len(same_X)) #dictionary of exercise indexes with unique X values (the first of each duplicate is taken as key, then the same exercise and any duplicates are value)\n","\n","#Randomly select 20% of exercises for validation\n","import random\n","\n","val_idx = random.sample(same_X.keys(), int(0.2*len(same_X.keys())))\n","train_idx = []\n","for i in same_X.keys():\n","  if i in val_idx:\n","    continue\n","  else:\n","    train_idx.append(i)\n","\n","print(len(val_idx), len(train_idx))\n","\n","train_set_X = []\n","train_set_Y = []\n","val_set_X = []\n","val_set_Y = []\n","\n","for i in same_X.keys():\n","  for j in same_X[i]:\n","    if i in train_idx:\n","      train_set_X.append(lhs_dataset_X[j])\n","      train_set_Y.append(lhs_dataset_Y[j])\n","    elif i in val_idx:\n","      val_set_X.append(lhs_dataset_X[j])\n","      val_set_Y.append(lhs_dataset_Y[j])\n","    else:\n","      print(\"Error - train/validation exercises not properly allocated\")\n","      break\n","\n","#Save exercises for use in other models for consistency\n","with open('/filepath/lhs_old_assumptions_trainX.txt', 'wb') as f:\n","  pickle.dump(train_set_X, f)\n","with open('/filepath/lhs_old_assumptions_trainY.txt', 'wb') as f:\n","  pickle.dump(train_set_Y, f)\n","with open('/filepath/lhs_old_assumptions_valX.txt', 'wb') as f:\n","  pickle.dump(val_set_X, f)\n","with open('/filepath/lhs_old_assumptions_valY.txt', 'wb') as f:\n","  pickle.dump(val_set_Y, f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rr4U7jfqZs1r"},"source":["#Load data\n","import pickle\n","\n","with open('/filepath/lhs_old_assumptions_trainX.txt', 'rb') as f:\n","  train_set_X = pickle.load(f)\n","with open('/filepath/lhs_old_assumptions_trainY.txt', 'rb') as f:\n","  train_set_Y = pickle.load(f)\n","with open('/filepath/lhs_old_assumptions_valX.txt', 'rb') as f:\n","  val_set_X = pickle.load(f)\n","with open('/filepath/lhs_old_assumptions_valY.txt', 'rb') as f:\n","  val_set_Y = pickle.load(f)\n","\n","#Pad data and prepare for training\n","import numpy as np\n","\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import TimeDistributed\n","from keras.layers import LSTM\n","from keras.layers import Bidirectional\n","from keras.layers import Masking\n","from keras import losses\n","from keras import optimizers\n","\n","from keras.callbacks import EarlyStopping\n","\n","train_pad_X = pad_sequences(train_set_X, value=-99, maxlen=128)\n","train_pad_Y = pad_sequences(train_set_Y, value=-99, maxlen=128)\n","val_pad_X = pad_sequences(val_set_X, value=-99, maxlen=128)\n","val_pad_Y = pad_sequences(val_set_Y, value=-99, maxlen=128)\n","\n","train_X_samples, timesteps, features = train_pad_X.shape\n","# print(lhs_X_padded.shape)\n","#shape should represent: (num samples, sample length = maxlength, 2 for X and Y)\n","\n","es = EarlyStopping(monitor='val_loss', patience = 5, verbose = 1)\n","\n","model = Sequential()\n","model.add(Masking(mask_value = -99, input_shape=(timesteps, features)))\n","model.add(Bidirectional(LSTM(5, return_sequences=True)))\n","model.add(Dense(4, activation='softmax'))\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"unF47Da-as2O"},"source":["#Train model\n","history = model.fit(x=train_pad_X, y=train_pad_Y, validation_data=(val_pad_X, val_pad_Y), batch_size = 5, epochs = 200, verbose = 1, shuffle=True, callbacks = [es])\n","\n","with open('/filepath/LSTM_PVE_S005_history.txt', 'wb') as f:\n","  pickle.dump(history.history, f)\n","model.save(\"/filepath/LSTM_PVE_S005\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V_KY4zQra5sd"},"source":["##Verify learning curves\n","import matplotlib.pyplot as plt\n","\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'val'], loc='upper left')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I-JxS2ura9E1"},"source":["#Prepare data for evaluation\n","from keras.models import load_model\n","from keras.preprocessing.sequence import pad_sequences\n","import pickle\n","\n","model = load_model(\"/filepath/LSTM_PVE_S005\")\n","\n","with open('/filepath/lhs_old_assumptions_valX.txt', 'rb') as f:\n","  val_set_X = pickle.load(f)\n","with open('/filepath/lhs_old_assumptions_valY.txt', 'rb') as f:\n","  val_set_Y = pickle.load(f)\n","\n","#Get length of each exercise\n","val_lens = [len(ex) for ex in val_set_X]\n","\n","#Pad each exercise to max\n","val_pad_X = pad_sequences(val_set_X, value=-99, maxlen=128)\n","val_pad_Y = pad_sequences(val_set_Y, value=-99, maxlen=128)\n","\n","#Predict classes on the test set\n","est_output = model.predict_classes(val_pad_X)\n","\n","#Remove padding in output\n","masked_output = []\n","for i in range(len(est_output)):\n"," masked_output.append([1+note for note in est_output[i][-val_lens[i]:]])\n","\n","#Convert validation GT's from one-hot to numerical encoding\n","val_output = []\n","for ex in val_set_Y:\n","  val_output.append([note.index(1)+1 for note in ex])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WK1PRfkmbK62"},"source":["##Sequence Accuracy\n","\n","#Strict approach\n","from keras import metrics\n","m = metrics.Accuracy()\n","perseq_acc = []\n","\n","for i in range(len(masked_output)):\n","  m.reset_state()\n","  m.update_state(masked_output[i], val_output[i])\n","  perseq_acc.append(m.result().numpy())\n","\n","print(sum(perseq_acc)/len(perseq_acc))\n","print(perseq_acc)\n","\n","#MGT approach\n","#Create dictionary. Keys are indices 0-1825 for each exercise. Values are lists of indices that have the same input\n","idx_map = {}\n","for idx, ex in enumerate(val_set_X):\n","  idx_map[idx] = []\n","  for idx_2, ex_2 in enumerate(val_set_X):\n","    if ex == ex_2:\n","      idx_map[idx].append(idx_2)\n","\n","#when evaluating each exercise, we want to compute the accuracy against all possible solutions, then pick the highest one as the GT.\n","mGT_accuracy_list = []\n","temp_acc = metrics.Accuracy()\n","\n","for i in range(len(masked_output)):\n","  mGT_idx = idx_map[i]\n","  temp_acc_list = []\n","  if len(mGT_idx) == 1:\n","    temp_acc.reset_state()\n","    temp_acc.update_state(masked_output[i], val_output[i])\n","    mGT_accuracy_list.append(temp_acc.result().numpy())\n","    continue\n","  else:\n","    for idx in mGT_idx:\n","      temp_acc.reset_state()\n","      temp_acc.update_state(masked_output[i], val_output[idx])\n","      temp_acc_list.append(temp_acc.result().numpy())\n","    mGT_accuracy_list.append(max(temp_acc_list))\n","\n","print(sum(mGT_accuracy_list)/len(mGT_accuracy_list))\n","print(mGT_accuracy_list)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6tmSsN43bcYl"},"source":["##Note Accuracy:\n","\n","#Strict approach\n","from keras import metrics\n","m = metrics.Accuracy()\n","\n","for i in range(len(masked_output)):\n","  m.update_state(masked_output[i], val_output[i])\n","print(m.result().numpy())\n","\n","#MGT approach\n","#Create dictionary. Keys are indices 0-1825 for each exercise. Values are lists of indices that have the same input\n","idx_map = {}\n","for idx, ex in enumerate(val_set_X):\n","  idx_map[idx] = []\n","  for idx_2, ex_2 in enumerate(val_set_X):\n","    if ex == ex_2:\n","      idx_map[idx].append(idx_2)\n","\n","mGT_accuracy_list = []\n","temp_acc = metrics.Accuracy()\n","\n","for i in range(len(masked_output)):\n","  ex_length = len(masked_output[i])\n","  mGT_idx = idx_map[i]\n","  temp_acc_list = []\n","  if len(mGT_idx) == 1:\n","    temp_acc.reset_state()\n","    temp_acc.update_state(masked_output[i], val_output[i])\n","    for j in range(ex_length):\n","      mGT_accuracy_list.append(temp_acc.result().numpy())\n","    continue\n","  else:\n","    for idx in mGT_idx:\n","      temp_acc.reset_state()\n","      temp_acc.update_state(masked_output[i], val_output[idx])\n","      for j in range(ex_length):\n","        temp_acc_list.append(temp_acc.result().numpy())\n","    mGT_accuracy_list.append(max(temp_acc_list))\n","\n","print(sum(mGT_accuracy_list)/len(mGT_accuracy_list))"],"execution_count":null,"outputs":[]}]}